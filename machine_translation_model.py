# -*- coding: utf-8 -*-
"""Machine_Translation_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YBF34xhHTcct88fn3nbheX41vdVG_5nx
"""

import os , sys
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

from tensorflow.keras.layers import Input,GRU, Dense, LSTM,Embedding
from tensorflow.keras.models import Model

BATCH_SIZE=64
EPOCHS=100
LATENT_DIM=256
NUM_SAMPLES=10000
MAX_SEQUENCE_LENGTH=100
MAX_NUM_WORDS=20000
EMBEDDING_DIM=100

! wget http://www.manythings.org/anki/fra-eng.zip

!unzip /content/fra-eng.zip

! head -5 fra.txt

input_texts= []
target_texts= []
target_texts_inputs= []

t = 0
for line in open('fra.txt',encoding='utf-8'):
  t += 1
  if t > NUM_SAMPLES:
      break
  if '\t' not in line:
     continue

input_text = line.rstrip().split('\t')[0]
translation = line.rstrip().split('\t')[1]

target_text = translation + ' <eos>'
target_text_input = '<sos> ' + translation

input_texts.append(input_text)
target_texts.append(target_text)

target_texts_inputs.append(target_text_input)

print("num samples :",len(input_texts))
print("num samples :",len(target_texts))

tokenizer_outputs=Tokenizer(num_words=MAX_NUM_WORDS,filters='')
tokenizer_outputs.fit_on_texts(target_texts + target_texts_inputs)
target_sequences=tokenizer_outputs.texts_to_sequences(target_texts)
target_sequences_inputs=tokenizer_outputs.texts_to_sequences(target_texts_inputs)

word2idx_outputs=tokenizer_outputs.word_index
print('Found %s unique output tokens'%(len(word2idx_outputs)))

num_words_output=len(word2idx_outputs)+1
max_len_target=max(len(s) for s in target_sequences)
max_len_target

tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS, filters='')
tokenizer_inputs.fit_on_texts(input_texts)
input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)

max_len_input = max(len(s) for s in input_sequences)
encoder_inputs=pad_sequences(input_sequences,maxlen=max_len_input)
print("encoder_inputs.shape:",encoder_inputs.shape)
print("encoder_inputs[0]:",encoder_inputs[0])

decoder_inputs=pad_sequences(target_sequences_inputs,maxlen=max_len_target,padding='post')
print("decoder_inputs[0]:",decoder_inputs[0])
print("decoder_inputs.shape:",decoder_inputs.shape)
decoder_targets=pad_sequences(target_sequences,maxlen=max_len_target,padding='post')

print('Loading word vectors...')
word2vec={}
with open(os.path.join('/content/fra.txt')) as f:
  for line in f:
    values=line.split()
    word=values[0]
    try:
        vec=np.asarray(values[1:],dtype='float32')
    except ValueError:
        print(f"Skipping line due to non-numerical value: {line}")
        continue  # Skip to the next line
    word2vec[word]=vec
print('Found %s word vectors.'%len(word2vec))

word2idx_inputs = tokenizer_inputs.word_index
print('Filling pre-trained embeddings...')
num_words=min(MAX_NUM_WORDS,len(word2idx_inputs)+1)
Embedding_matrix=np.zeros((num_words,EMBEDDING_DIM))
for word,i in word2idx_inputs.items():
  if i<MAX_NUM_WORDS:
    Embedding_vector=word2vec.get(word)
    if Embedding_vector is not None:
      Embedding_matrix[i]=Embedding_vector

embedding_layer=Embedding(
    num_words,
    EMBEDDING_DIM,
    weights=[Embedding_matrix],
    input_length=max_len_input
)

decoder_targets_one_hot=np.zeros((
    len(input_texts),
    max_len_target,
    num_words_output
),dtype='float32')

for i,d in enumerate(decoder_targets):
  for t,word in enumerate(d):
    if word>0:
      decoder_targets_one_hot[i,t,word]=1

encoder_inputs_placeholder=Input(shape=(max_len_input,))
x=embedding_layer(encoder_inputs_placeholder)
encoder=LSTM(LATENT_DIM,return_state=True)
encoder_outputs,h,c=encoder(x)
encoder_states=[h,c]

decoder_inputs_placeholder=Input(shape=(max_len_target,))
decoder_embedding=Embedding(num_words_output,LATENT_DIM)
decoder_inputs_x=decoder_embedding(decoder_inputs_placeholder)

decoder_lstm=LSTM(LATENT_DIM,return_sequences=True,return_state=True)
decoder_outputs,_,_=decoder_lstm(decoder_inputs_x,initial_state=encoder_states)

decoder_dense=Dense(num_words_output,activation='softmax')
decoder_outputs=decoder_dense(decoder_outputs)

model=Model([encoder_inputs_placeholder,decoder_inputs_placeholder],decoder_outputs)
model.summary()

model.compile(
    optimizer='rmsprop',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

r=model.fit(
    [encoder_inputs,decoder_inputs],
    decoder_targets_one_hot,
    batch_size=BATCH_SIZE,
    epochs=5,

)

plt.plot(r.history['loss'],label='loss')
plt.plot(r.history['accuracy'],label='accuracy')
plt.legend()
plt.show()

plt.plot(r.history['loss'],label='loss')
plt.legend()
plt.show()

encoder_model=Model(encoder_inputs_placeholder,encoder_states)

decoder_state_input_h=Input(shape=(LATENT_DIM,))
decoder_state_input_c=Input(shape=(LATENT_DIM,))
decoder_states_inputs=[decoder_state_input_h,decoder_state_input_c]
decoder_inputs_single=Input(shape=(1,))
decoder_inputs_single_x=decoder_embedding(decoder_inputs_single)
decoder_outputs,h,c=decoder_lstm(decoder_inputs_single_x,initial_state=decoder_states_inputs)

decoder_states=[h,c]
decoder_outputs=decoder_dense(decoder_outputs)
decoder_model=Model(
    [decoder_inputs_single]+decoder_states_inputs,
    [decoder_outputs]+decoder_states
)

idx2word_eng={v:k for k,v in word2idx_inputs.items()}
idx2word_trans={v:k for k,v in word2idx_outputs.items()}

def decode_sequence(input_seq):
  states_value=encoder_model.predict(input_seq)
  # Initialize target_seq as a NumPy array
  target_seq=np.zeros((1,1))
  target_seq[0,0]=word2idx_outputs['<sos>']
  eos=word2idx_outputs['<eos>']
  output_sentence=[]
  for _ in range(max_len_target):
    output_tokens,h,c=decoder_model.predict([target_seq]+states_value)

    # Update target_seq for the next iteration
    idx = np.argmax(output_tokens[0, 0, :])
    target_seq[0, 0] = idx  # Assign the predicted index to target_seq

    if eos==idx:
      break
    word=''
    if idx>0:
      word=idx2word_trans[idx]
      output_sentence.append(word)

  # No need for the lines causing the error
  # idx=0
  # target_sequences[0][0] = idx
  # states_value = [h, c]

  return ''.join(output_sentence) # Return the generated sentence

while True:
  i=np.random.choice(len(input_texts))
  input_seq=encoder_inputs[i:i+1]
  translation=decode_sequence(input_seq)
  print('-')
  print('Input:',input_texts[i])
  print('Translation:',translation)


  ans=input("Continue? [Y/n]")
  if ans and ans.lower().startswith('n'):
    break